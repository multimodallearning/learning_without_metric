{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import cc3d\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data_3d/' # define data directory\n",
    "CASES = ['-', 'TCGA-B8-5158' , '-', 'TCGA-B8-5545', '-', 'TCGA-B8-5551', '-', 'TCGA-BP-5006', '-', 'TCGA-DD-A1EI', '-', 'TCGA-DD-A4NJ', '-', 'TCGA-G7-7502', '-', 'TCGA-G7-A8LC']\n",
    "FOLD = 0\n",
    "if FOLD == 0:\n",
    "    TEST_CASES = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "elif FOLD == 1:\n",
    "    TEST_CASES = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "TRAIN_CASES = [i for i in range(len(CASES)) if not i in TEST_CASES]\n",
    "D, H, W = 192, 160, 192\n",
    "dtype = torch.float32\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    print('# parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    \n",
    "def dice_coeff(outputs, labels, max_label):\n",
    "    dice = torch.zeros(max_label-1)\n",
    "    for label in range(1, max_label):\n",
    "        iflat = (outputs==label).reshape(-1).float()\n",
    "        tflat = (labels==label).reshape(-1).float()\n",
    "        intersection = torch.mean(iflat * tflat)\n",
    "        dice[label-1] = (2. * intersection) / (1e-8 + torch.mean(iflat) + torch.mean(tflat))\n",
    "    return dice\n",
    "\n",
    "\n",
    "def find_rigid_3d(x, y):\n",
    "    x_mean = x[:, :3].mean(0)\n",
    "    y_mean = y[:, :3].mean(0)\n",
    "    u, s, v = torch.svd(torch.matmul((x[:, :3]-x_mean).t(), (y[:, :3]-y_mean)))\n",
    "    m = torch.eye(v.shape[0], v.shape[0]).to(x.device)\n",
    "    m[-1,-1] = torch.det(torch.matmul(v, u.t()))\n",
    "    rotation = torch.matmul(torch.matmul(v, m), u.t())\n",
    "    translation = y_mean - torch.matmul(rotation, x_mean)\n",
    "    T = torch.eye(4).to(x.device)\n",
    "    T[:3,:3] = rotation\n",
    "    T[:3, 3] = translation\n",
    "    return T\n",
    "\n",
    "\n",
    "def generate_random_rigid_3d(strength=.3):\n",
    "    x = torch.randn(12,3).to(device)\n",
    "    y = x + strength*torch.randn(12,3).to(device)\n",
    "    return find_rigid_3d(x, y)\n",
    "\n",
    "\n",
    "def compute_datacost_grid(mask_fix, mind_fix, mind_mov, grid_step, disp_radius, disp_step, beta=15):\n",
    "    _, _, D, H, W = mask_fix.shape\n",
    "\n",
    "    grid_pts = F.affine_grid(.925 * torch.eye(3,4).unsqueeze(0).to(device), (1, 1, H//grid_step, W//grid_step, D//grid_step), align_corners=True).view(1,1,1,-1,3)\n",
    "    mask_bg = F.grid_sample(mask_fix, grid_pts, align_corners=True)\n",
    "    grid_pts = grid_pts[:, :, :, mask_bg.view(-1)>0.5, :]\n",
    "\n",
    "    cost = ssd(grid_pts.view(1, -1, 3), mind_fix, mind_mov, (D,H,W), disp_radius, disp_step, disp_radius+1)\n",
    "    disp = torch.stack(torch.meshgrid(torch.arange(- disp_step * disp_radius, disp_step * disp_radius  + 1, disp_step),\n",
    "                                      torch.arange(- disp_step * disp_radius, disp_step * disp_radius  + 1, disp_step),\n",
    "                                      torch.arange(- disp_step * disp_radius, disp_step * disp_radius  + 1, disp_step))).permute(1, 2, 3, 0).contiguous().view(1, -1, 3).float()\n",
    "    disp = (disp.flip(-1) * 2 / (torch.tensor([W, H, D]) - 1)).to(device)\n",
    "    \n",
    "    ssd_val, ssd_idx = torch.min(cost.squeeze(), 1)\n",
    "    idx_best = torch.sort(ssd_val, dim=0, descending=False)[1][:grid_pts.shape[3]//2]\n",
    "    disp_best = torch.sum(torch.softmax(-beta*cost.squeeze(0).unsqueeze(2),1) * disp, 1)\n",
    "    disp_best = disp_best[idx_best,:]\n",
    "    fixed_pts = torch.cat((grid_pts[0,0,0,idx_best,:], torch.ones(idx_best.size(0),1).to(device)),1)\n",
    "    moving_pts = torch.cat((grid_pts[0,0,0,idx_best,:] + disp_best, torch.ones(idx_best.size(0),1).to(device)),1)\n",
    "    return fixed_pts,moving_pts\n",
    "\n",
    "\n",
    "def least_trimmed_rigid(fixed_pts, moving_pts, iter=5):\n",
    "    idx = torch.arange(fixed_pts.shape[0]).to(fixed_pts.device)\n",
    "    for i in range(iter):\n",
    "        x = find_rigid_3d(fixed_pts[idx,:], moving_pts[idx,:]).t()\n",
    "        residual = torch.sqrt(torch.sum(torch.pow(moving_pts - torch.mm(fixed_pts, x), 2), 1))\n",
    "        _, idx = torch.topk(residual, fixed_pts.shape[0]//2, largest=False)\n",
    "    return x.t()\n",
    "\n",
    "\n",
    "def ssd(kpts_fixed, feat_fixed, feat_moving, orig_shape, disp_radius=16, disp_step=2, patch_radius=2, alpha=1.5, unroll_factor=50):\n",
    "    _, N, _ = kpts_fixed.shape\n",
    "    device = kpts_fixed.device\n",
    "    D, H, W = orig_shape\n",
    "    C = feat_fixed.shape[1]\n",
    "    dtype = feat_fixed.dtype\n",
    "    \n",
    "    patch_step = disp_step # same stride necessary for fast implementation\n",
    "    patch = torch.stack(torch.meshgrid(torch.arange(0, 2 * patch_radius + 1, patch_step),\n",
    "                                       torch.arange(0, 2 * patch_radius + 1, patch_step),\n",
    "                                       torch.arange(0, 2 * patch_radius + 1, patch_step))).permute(1, 2, 3, 0).contiguous().view(1, 1, -1, 1, 3).float() - patch_radius\n",
    "    patch = (patch.flip(-1) * 2 / (torch.tensor([W, H, D]) - 1)).to(dtype).to(device)\n",
    "    \n",
    "    patch_width = round(patch.shape[2] ** (1.0 / 3))\n",
    "    \n",
    "    if patch_width % 2 == 0:\n",
    "        pad = [(patch_width - 1) // 2, (patch_width - 1) // 2 + 1]\n",
    "    else:\n",
    "        pad = [(patch_width - 1) // 2, (patch_width - 1) // 2]\n",
    "    \n",
    "    disp = torch.stack(torch.meshgrid(torch.arange(- disp_step * (disp_radius + ((pad[0] + pad[1]) / 2)), (disp_step * (disp_radius + ((pad[0] + pad[1]) / 2))) + 1, disp_step),\n",
    "                                      torch.arange(- disp_step * (disp_radius + ((pad[0] + pad[1]) / 2)), (disp_step * (disp_radius + ((pad[0] + pad[1]) / 2))) + 1, disp_step),\n",
    "                                      torch.arange(- disp_step * (disp_radius + ((pad[0] + pad[1]) / 2)), (disp_step * (disp_radius + ((pad[0] + pad[1]) / 2))) + 1, disp_step))).permute(1, 2, 3, 0).contiguous().view(1, 1, -1, 1, 3).float()\n",
    "    disp = (disp.flip(-1) * 2 / (torch.tensor([W, H, D]) - 1)).to(dtype).to(device)\n",
    "    \n",
    "    disp_width = disp_radius * 2 + 1\n",
    "    ssd = torch.zeros(1, N, disp_width ** 3).to(device)\n",
    "    split = np.array_split(np.arange(N), unroll_factor)\n",
    "    for i in range(unroll_factor):\n",
    "        feat_fixed_patch = F.grid_sample(feat_fixed, kpts_fixed[:, split[i], :].view(1, -1, 1, 1, 3).to(dtype) + patch, padding_mode='border', align_corners=True)\n",
    "        feat_moving_disp = F.grid_sample(feat_moving, kpts_fixed[:, split[i], :].view(1, -1, 1, 1, 3).to(dtype) + disp, padding_mode='border', align_corners=True)        \n",
    "        corr = F.conv3d(feat_moving_disp.view(1, -1, disp_width + pad[0] + pad[1], disp_width + pad[0] + pad[1], disp_width + pad[0] + pad[1]), feat_fixed_patch.view(-1, 1, patch_width, patch_width, patch_width), groups=C * split[i].shape[0]).view(C, split[i].shape[0], -1)\n",
    "        patch_sum = (feat_fixed_patch ** 2).squeeze(0).squeeze(3).sum(dim=2, keepdims=True)\n",
    "        disp_sum = (patch_width ** 3) * F.avg_pool3d((feat_moving_disp ** 2).view(C, -1, disp_width + pad[0] + pad[1], disp_width + pad[0] + pad[1], disp_width + pad[0] + pad[1]), patch_width, stride=1).view(C, split[i].shape[0], -1)\n",
    "        ssd[0, split[i], :] = ((- 2 * corr + patch_sum + disp_sum)).sum(0)\n",
    "    \n",
    "    ssd *= (alpha / (patch_width ** 3))\n",
    "    \n",
    "    return ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_case(case, plot=False):\n",
    "    img_fix = torch.from_numpy(nib.load(os.path.join(DATA_DIR, '{}_MR2mm_crop.nii.gz'.format(CASES[case]))).get_fdata()).to(dtype)\n",
    "    img_fix -= img_fix.mean()\n",
    "    img_fix /= img_fix.std()\n",
    "    img_mov = (torch.from_numpy(nib.load(os.path.join(DATA_DIR, '{}_CT2mm_crop.nii.gz'.format(CASES[case]))).get_fdata()).to(dtype).clip_(-1000, 1500)+1000)/2500\n",
    "    seg_fix = torch.from_numpy(nib.load(os.path.join(DATA_DIR, '{}_MR2mm_segcrop.nii.gz'.format(CASES[case]))).get_fdata()).long()\n",
    "    seg_mov = torch.from_numpy(nib.load(os.path.join(DATA_DIR, '{}_CT2mm_segcrop.nii.gz'.format(CASES[case]))).get_fdata()).long()\n",
    "    \n",
    "    mask_fix = img_fix < -0.25\n",
    "    mask_fix = cc3d.connected_components(mask_fix.numpy()) == 0\n",
    "    mask_fix = ndimage.binary_erosion(ndimage.binary_dilation(mask_fix, iterations=5), iterations=5)\n",
    "    mask_fix = torch.from_numpy(mask_fix)\n",
    "    mask_mov = img_mov < 0.05\n",
    "    mask_mov = cc3d.connected_components(mask_mov.numpy()) == 0\n",
    "    mask_mov = ndimage.binary_erosion(ndimage.binary_dilation(mask_mov, iterations=5), iterations=5)\n",
    "    mask_mov = torch.from_numpy(mask_mov)\n",
    "    \n",
    "    if plot:\n",
    "        cmap = plt.get_cmap('Set1')\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img_fix[:, :, W//2], cmap='gray')\n",
    "        seg_fix_plot = cmap(seg_fix[:, :, W//2]/5.)\n",
    "        seg_fix_plot[:, :, 3] = seg_fix[:, :, W//2] != 0\n",
    "        plt.imshow(seg_fix_plot, alpha=0.5)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_mov[:, :, W//2], cmap='gray')\n",
    "        seg_mov_plot = cmap(seg_mov[:, :, W//2]/5.)\n",
    "        seg_mov_plot[:, :, 3] = seg_mov[:, :, W//2] != 0\n",
    "        plt.imshow(seg_mov_plot, alpha=0.5)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    return img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov\n",
    "\n",
    "dice_all = 0\n",
    "for case in TRAIN_CASES:\n",
    "    print('Case: ', CASES[case])\n",
    "    img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov = load_case(case, plot=False)\n",
    "    dice = dice_coeff(seg_fix, seg_mov, 5)\n",
    "    dice_all += dice\n",
    "    print('Initial Dice: {:.2f}, {:.2f}, {:.2f}, {:.2f} (mean: {:.2f})'.format(*(dice.tolist()), dice.mean().item()))\n",
    "    print('--')\n",
    "    print()\n",
    "dice_all /= len(TRAIN_CASES)\n",
    "print('Initial Dice (all): {:.2f}, {:.2f}, {:.2f}, {:.2f} (mean: {:.2f})'.format(*(dice_all.tolist()), dice_all.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_fix1_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).float().pin_memory()\n",
    "imgs_mov2_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).float().pin_memory()\n",
    "imgs_mov3_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).float().pin_memory()\n",
    "segs_fix1_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).int().pin_memory()\n",
    "segs_mov2_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).int().pin_memory()\n",
    "masks_fix1_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).bool().pin_memory()\n",
    "R21s = torch.zeros(len(TRAIN_CASES), 8, 4, 4).float().pin_memory()\n",
    "R23s = torch.zeros(len(TRAIN_CASES), 8, 4, 4).float().pin_memory()\n",
    "\n",
    "torch.manual_seed(60)\n",
    "\n",
    "for i, case in enumerate(TRAIN_CASES):\n",
    "    print('process case', i)\n",
    "    img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov = load_case(case)\n",
    "    img_fix = img_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
    "    img_mov = img_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
    "    seg_fix = seg_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
    "    seg_mov = seg_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
    "    mask_fix = mask_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    imgs_mov2_train[i:i+1] = img_mov\n",
    "    segs_mov2_train[i:i+1] = seg_mov\n",
    "    \n",
    "    for j in range(8):\n",
    "        R = generate_random_rigid_3d()\n",
    "        grid = F.affine_grid(R[:3,:4].unsqueeze(0), (1,1,D,H,W))\n",
    "        img_fix_ = F.grid_sample(img_fix, grid, padding_mode='border')\n",
    "        seg_fix_ = F.grid_sample(F.one_hot(seg_fix[0, 0]).permute(3, 0, 1, 2).unsqueeze(0).float(), grid).argmax(1, keepdim=True).int()\n",
    "        mask_fix_ = F.grid_sample(mask_fix.float(), grid)>0.5\n",
    "        \n",
    "        imgs_fix1_train[i:i+1, j:j+1] = img_fix_\n",
    "        segs_fix1_train[i:i+1, j:j+1] = seg_fix_\n",
    "        masks_fix1_train[i:i+1, j:j+1] = mask_fix_\n",
    "        R21s[i:i+1, j:j+1] = R\n",
    "        \n",
    "        R = generate_random_rigid_3d()\n",
    "        grid = F.affine_grid(R[:3,:4].unsqueeze(0), (1,1,D,H,W))\n",
    "        img_mov_ = F.grid_sample(img_mov, grid, padding_mode='border')\n",
    "        seg_mov_ = F.grid_sample(F.one_hot(seg_mov[0, 0]).permute(3, 0, 1, 2).unsqueeze(0).float(), grid).argmax(1, keepdim=True).int()\n",
    "        \n",
    "        imgs_mov3_train[i:i+1, j:j+1] = img_mov_\n",
    "        R23s[i:i+1, j:j+1] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_step = 12\n",
    "disp_radius = 4\n",
    "disp_step = 5\n",
    "beta=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModalityNet(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super(ModalityNet, self).__init__()\n",
    "        \n",
    "        base = 8\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(1, base, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(base, base, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(base, base*2, 3, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base*2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(base*2, base*2, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base*2),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "    \n",
    "class SharedNet(nn.Module):\n",
    "    def __init__(self, base, out_channels):\n",
    "        super(SharedNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(base*2, base*2, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base*2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(base*2, base*2, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base*2),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(base*2, base*4, 3, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base*4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(base*4, base*4, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(base*4),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(base*4, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureNet, self).__init__()\n",
    "        \n",
    "        base = 8\n",
    "        out_channels = 16\n",
    "        \n",
    "        self.modality1_net = ModalityNet(base)\n",
    "        self.modality2_net = ModalityNet(base)\n",
    "        self.shared_net = SharedNet(base, out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.modality1_net(x)\n",
    "        y = self.modality2_net(y)\n",
    "        x = self.shared_net(x)\n",
    "        y = self.shared_net(y)\n",
    "        return self.sigmoid(x), self.sigmoid(y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(net, img_fix, img_mov, mask_fix):\n",
    "    feat_fix, feat_mov = net(img_fix, img_mov)\n",
    "    fixed_pts, moving_pts = compute_datacost_grid(mask_fix.float(), feat_fix, feat_mov, grid_step, disp_radius, disp_step, beta)\n",
    "    R = least_trimmed_rigid(fixed_pts, moving_pts)\n",
    "    return R\n",
    "\n",
    "num_epochs = 100\n",
    "init_lr = 0.001\n",
    "\n",
    "net = FeatureNet().to(device)\n",
    "parameter_count(net)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=init_lr)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=init_lr/(10**2))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses = torch.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    running_loss = 0\n",
    "    rand_idx = torch.randperm(len(TRAIN_CASES))\n",
    "    for idx in rand_idx:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        rand_idx1 = torch.randint(8, (1,))[0]\n",
    "        rand_idx2 = torch.randint(8, (1,))[0]\n",
    "        img_fix1 = imgs_fix1_train[idx:idx+1, rand_idx1:rand_idx1+1].to(device,non_blocking=True)\n",
    "        img_mov2 = imgs_mov2_train[idx:idx+1].to(device,non_blocking=True)\n",
    "        img_mov3 = imgs_mov3_train[idx:idx+1, rand_idx2:rand_idx2+1].to(device,non_blocking=True)\n",
    "        seg_fix1 = segs_fix1_train[idx:idx+1, rand_idx1:rand_idx1+1].long().to(device,non_blocking=True)\n",
    "        seg_mov2 = segs_mov2_train[idx:idx+1].to(device,non_blocking=True).long()\n",
    "        mask_fix1 = masks_fix1_train[idx:idx+1, rand_idx1:rand_idx1+1].to(device,non_blocking=True)\n",
    "        R23 = R23s[idx, rand_idx2].to(device,non_blocking=True)\n",
    "\n",
    "        R21 = predict(net, img_fix1, img_mov2, mask_fix1)\n",
    "        R31 = predict(net, img_fix1, img_mov3, mask_fix1)\n",
    "        \n",
    "        R23_ = torch.mm(R21,R31.inverse())\n",
    "        \n",
    "        grid23 = F.affine_grid(R23[:3].unsqueeze(0), (1,1,D,H,W))\n",
    "        grid23_ = F.affine_grid(R23_[:3].unsqueeze(0), (1,1,D,H,W))\n",
    "\n",
    "        loss = criterion(grid23, grid23_)\n",
    "      \n",
    "        if epoch%10==9:\n",
    "            grid21 = F.affine_grid(R21[:3,:4].unsqueeze(0), (1,1,D,H,W))\n",
    "            seg_mov2_warped = F.grid_sample(F.one_hot(seg_mov2, 5).view(1, D, H, W, -1).permute(0, 4, 1, 2, 3).float(), grid21, mode='bilinear')\n",
    "            print('epoch (train): {:02d} -- mean dice case {:01d}: {:.2f}'.format(epoch, idx, dice_coeff(seg_fix1, seg_mov2_warped.argmax(1, keepdim=True), 5).mean().item()))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    running_loss /= len(TRAIN_CASES)\n",
    "    losses[epoch] = running_loss\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print('epoch (train): {:02d} -- loss: {:.4f} -- time(s): {:.1f}'.format(epoch, running_loss, t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "torch.save(net.cpu().state_dict(), 'net3d_cycle_fold{}.pth'.format(FOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
